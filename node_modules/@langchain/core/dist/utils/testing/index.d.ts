import { BaseCallbackConfig, CallbackManagerForLLMRun } from "../../callbacks/manager.js";
import { BaseChatMessageHistory, BaseListChatMessageHistory } from "../../chat_history.js";
import { Document } from "../../documents/document.js";
import { BaseChatModel, BaseChatModelParams } from "../../language_models/chat_models.js";
import { BaseLLMParams, LLM } from "../../language_models/llms.js";
import { BaseMessage, AIMessage } from "../../messages/index.js";
import { BaseOutputParser } from "../../output_parsers/base.js";
import { GenerationChunk, type ChatResult, ChatGenerationChunk } from "../../outputs.js";
import { BaseRetriever } from "../../retrievers.js";
import { Runnable } from "../../runnables/base.js";
/**
 * Parser for comma-separated values. It splits the input text by commas
 * and trims the resulting values.
 */
export declare class FakeSplitIntoListParser extends BaseOutputParser<string[]> {
    lc_namespace: string[];
    getFormatInstructions(): string;
    parse(text: string): Promise<string[]>;
}
export declare class FakeRunnable extends Runnable<string, Record<string, any>> {
    lc_namespace: string[];
    returnOptions?: boolean;
    constructor(fields: {
        returnOptions?: boolean;
    });
    invoke(input: string, options?: Partial<BaseCallbackConfig>): Promise<Record<string, any>>;
}
export declare class FakeLLM extends LLM {
    response?: string;
    thrownErrorString?: string;
    constructor(fields: {
        response?: string;
        thrownErrorString?: string;
    } & BaseLLMParams);
    _llmType(): string;
    _call(prompt: string, _options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<string>;
}
export declare class FakeStreamingLLM extends LLM {
    sleep?: number;
    constructor(fields: {
        sleep?: number;
    } & BaseLLMParams);
    _llmType(): string;
    _call(prompt: string): Promise<string>;
    _streamResponseChunks(input: string): AsyncGenerator<GenerationChunk, void, unknown>;
}
export declare class FakeChatModel extends BaseChatModel {
    _combineLLMOutput(): never[];
    _llmType(): string;
    _generate(messages: BaseMessage[], options?: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;
}
export declare class FakeRetriever extends BaseRetriever {
    lc_namespace: string[];
    output: Document<Record<string, any>>[];
    constructor(fields?: {
        output: Document[];
    });
    _getRelevantDocuments(_query: string): Promise<Document<Record<string, any>>[]>;
}
/**
 * Interface for the input parameters specific to the Fake List Chat model.
 */
export interface FakeChatInput extends BaseChatModelParams {
    /** Responses to return */
    responses: string[];
    /** Time to sleep in milliseconds between responses */
    sleep?: number;
}
/**
 * A fake Chat Model that returns a predefined list of responses. It can be used
 * for testing purposes.
 * @example
 * ```typescript
 * const chat = new FakeListChatModel({
 *   responses: ["I'll callback later.", "You 'console' them!"]
 * });
 *
 * const firstMessage = new HumanMessage("You want to hear a JavaScript joke?");
 * const secondMessage = new HumanMessage("How do you cheer up a JavaScript developer?");
 *
 * // Call the chat model with a message and log the response
 * const firstResponse = await chat.call([firstMessage]);
 * console.log({ firstResponse });
 *
 * const secondResponse = await chat.call([secondMessage]);
 * console.log({ secondResponse });
 * ```
 */
export declare class FakeListChatModel extends BaseChatModel {
    static lc_name(): string;
    responses: string[];
    i: number;
    sleep?: number;
    constructor({ responses, sleep }: FakeChatInput);
    _combineLLMOutput(): never[];
    _llmType(): string;
    _generate(_messages: BaseMessage[], options?: this["ParsedCallOptions"]): Promise<ChatResult>;
    _formatGeneration(text: string): {
        message: AIMessage;
        text: string;
    };
    _streamResponseChunks(_messages: BaseMessage[], _options: this["ParsedCallOptions"], _runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _sleepIfRequested(): Promise<void>;
    _sleep(): Promise<void>;
    _createResponseChunk(text: string): ChatGenerationChunk;
    _currentResponse(): string;
    _incrementResponse(): void;
}
export declare class FakeChatMessageHistory extends BaseChatMessageHistory {
    lc_namespace: string[];
    messages: Array<BaseMessage>;
    constructor();
    getMessages(): Promise<BaseMessage[]>;
    addMessage(message: BaseMessage): Promise<void>;
    addUserMessage(message: string): Promise<void>;
    addAIChatMessage(message: string): Promise<void>;
    clear(): Promise<void>;
}
export declare class FakeListChatMessageHistory extends BaseListChatMessageHistory {
    lc_namespace: string[];
    messages: Array<BaseMessage>;
    constructor();
    addMessage(message: BaseMessage): Promise<void>;
    getMessages(): Promise<BaseMessage[]>;
}
